{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb6255fd-0f5e-4d39-98f7-238105403b66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized groups: ['Neg' 'Fungi' 'Pos']\n",
      "Total spectra: 102600\n",
      "Train spectra: 73200, Val spectra: 18600, Test spectra: 10800\n",
      "\n",
      "Train set species counts:\n",
      " Eco     4500\n",
      "Api     3900\n",
      "CaAl    3600\n",
      "Ano     3600\n",
      "CaTr    3000\n",
      "CaKr    3000\n",
      "Kox     3000\n",
      "CaPa    3000\n",
      "Efa     2700\n",
      "CaGl    2700\n",
      "CaGu    2700\n",
      "CrNe    2400\n",
      "Sau     1800\n",
      "Pae     1800\n",
      "Mmo     1800\n",
      "Sma     1800\n",
      "Efm     1800\n",
      "Sep     1800\n",
      "Shl     1800\n",
      "Bce     1800\n",
      "Kpn     1500\n",
      "Sho     1500\n",
      "Stm     1500\n",
      "Sca     1500\n",
      "Ecl     1500\n",
      "Aba     1500\n",
      "Eae     1500\n",
      "Bfg     1200\n",
      "Cdi     1200\n",
      "Cfr     1200\n",
      "Lmo     1200\n",
      "Pmi     1200\n",
      "Spn     1200\n",
      "Sgc     1200\n",
      "Sal      900\n",
      "Svi      900\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Val set species counts:\n",
      " Kox     1200\n",
      "Eco     1200\n",
      "CrNe    1200\n",
      "CaGu     900\n",
      "CaKr     900\n",
      "CaTr     900\n",
      "CaGl     900\n",
      "Pmi      900\n",
      "Cfr      900\n",
      "Svi      900\n",
      "CaPa     600\n",
      "Stm      600\n",
      "Aba      600\n",
      "Api      600\n",
      "Ano      600\n",
      "Sgc      600\n",
      "Eae      600\n",
      "Efa      600\n",
      "Ecl      600\n",
      "Cdi      600\n",
      "Kpn      300\n",
      "CaAl     300\n",
      "Sma      300\n",
      "Mmo      300\n",
      "Bfg      300\n",
      "Sho      300\n",
      "Sca      300\n",
      "Sal      300\n",
      "Spn      300\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Test set species counts:\n",
      " Kpn     300\n",
      "Eco     300\n",
      "Bce     300\n",
      "Stm     300\n",
      "Aba     300\n",
      "CaAl    300\n",
      "CaKr    300\n",
      "CaPa    300\n",
      "CaTr    300\n",
      "CaGl    300\n",
      "CrNe    300\n",
      "CaGu    300\n",
      "Bfg     300\n",
      "Cdi     300\n",
      "Cfr     300\n",
      "Eae     300\n",
      "Ecl     300\n",
      "Efa     300\n",
      "Efm     300\n",
      "Pae     300\n",
      "Lmo     300\n",
      "Mmo     300\n",
      "Pmi     300\n",
      "Sal     300\n",
      "Sau     300\n",
      "Sca     300\n",
      "Sep     300\n",
      "Sgc     300\n",
      "Shl     300\n",
      "Sho     300\n",
      "Sma     300\n",
      "Spn     300\n",
      "Svi     300\n",
      "Kox     300\n",
      "Ano     300\n",
      "Api     300\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Data Loading, Preprocessing, and Stratified Group Split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "# 1. Load the dataset, force Sample and AB as strings to avoid mixed types\n",
    "df = pd.read_csv(\n",
    "    './data300.csv',\n",
    "    dtype={'Sample': str, 'AB': str},\n",
    "    low_memory=False,\n",
    ")\n",
    "df['Sample'] = df['Sample'].astype(str)\n",
    "\n",
    "# --- Normalize the Group column to exactly \"Neg\", \"Pos\", or \"Fungi\" ---\n",
    "# 1) lower‐case and strip whitespace\n",
    "df['Group'] = df['Group'].str.strip().str.lower()\n",
    "\n",
    "# 2) map every variant you have to our three canonical keys\n",
    "df['Group'] = df['Group'].map({\n",
    "    'neg': 'Neg',\n",
    "    'gram‐negative': 'Neg',\n",
    "    'gram-negative bacteria': 'Neg',\n",
    "    'pos': 'Pos',\n",
    "    'gram‐positive': 'Pos',\n",
    "    'gram-positive bacteria': 'Pos',\n",
    "    'yeast': 'Fungi',           # <-- your “fungi” are tagged as “Yeast”\n",
    "    'fungi': 'Fungi',           # in case any got through\n",
    "})\n",
    "\n",
    "# sanity check\n",
    "print(\"Normalized groups:\", df['Group'].unique())\n",
    "\n",
    "# 2. Identify intensity columns (first 5 columns are metadata)\n",
    "intensity_cols = df.columns[5:]\n",
    "\n",
    "# 3. Min-max normalize each spectrum (each row) across the intensity columns\n",
    "X_values = df[intensity_cols].to_numpy(dtype=np.float32)\n",
    "mins = X_values.min(axis=1, keepdims=True)\n",
    "maxs = X_values.max(axis=1, keepdims=True)\n",
    "ranges = np.where(maxs - mins == 0, 1.0, maxs - mins)\n",
    "X_norm = (X_values - mins) / ranges  # shape (N, num_wavelengths)\n",
    "\n",
    "# 4. Encode species labels into integer classes\n",
    "species_labels = df['ID'].astype(str).values\n",
    "unique_species = sorted(np.unique(species_labels))\n",
    "label_to_idx = {lab: i for i, lab in enumerate(unique_species)}\n",
    "y_full = np.array([label_to_idx[s] for s in species_labels], dtype=np.int64)\n",
    "\n",
    "# 5. Build a unique sample key per biological sample: \"<Species>_<SampleID>\"\n",
    "df['sample_key'] = df['ID'].astype(str) + '_' + df['Sample'].astype(str)\n",
    "\n",
    "# 6. Stratified selection of one sample_key per species for the test set\n",
    "species_to_keys = defaultdict(list)\n",
    "for key, species in zip(df['sample_key'], df['ID']):\n",
    "    species_to_keys[species].append(key)\n",
    "\n",
    "np.random.seed(42)\n",
    "test_keys = []\n",
    "for species, keys in species_to_keys.items():\n",
    "    if len(keys) >= 2:\n",
    "        # randomly select one key for test\n",
    "        test_keys.append(np.random.choice(keys))\n",
    "    # species with only one sample_key remain in train/val\n",
    "\n",
    "# 7. Remaining keys for train+val\n",
    "all_keys = set(df['sample_key'])\n",
    "train_val_keys = list(all_keys - set(test_keys))\n",
    "np.random.shuffle(train_val_keys)\n",
    "\n",
    "# 8. Split train+val keys into 80% train, 20% val\n",
    "n_train_val = len(train_val_keys)\n",
    "n_train = int(0.8 * n_train_val)\n",
    "train_keys = train_val_keys[:n_train]\n",
    "val_keys   = train_val_keys[n_train:]\n",
    "\n",
    "# 9. Build boolean masks for each split based on sample_key\n",
    "train_mask = df['sample_key'].isin(train_keys)\n",
    "val_mask   = df['sample_key'].isin(val_keys)\n",
    "test_mask  = df['sample_key'].isin(test_keys)\n",
    "\n",
    "# 10. Slice out X and y for each split\n",
    "X_train, y_train = X_norm[train_mask], y_full[train_mask]\n",
    "X_val,   y_val   = X_norm[val_mask],   y_full[val_mask]\n",
    "X_test,  y_test  = X_norm[test_mask],  y_full[test_mask]\n",
    "\n",
    "# 11. Sanity checks: counts and species coverage\n",
    "print(f\"Total spectra: {len(df)}\")\n",
    "print(f\"Train spectra: {X_train.shape[0]}, Val spectra: {X_val.shape[0]}, Test spectra: {X_test.shape[0]}\")\n",
    "\n",
    "# Count per species in each split\n",
    "import pandas as pd\n",
    "train_counts = pd.Series(y_train).map(lambda i: unique_species[i]).value_counts()\n",
    "val_counts   = pd.Series(y_val).map(lambda i: unique_species[i]).value_counts()\n",
    "test_counts  = pd.Series(y_test).map(lambda i: unique_species[i]).value_counts()\n",
    "\n",
    "print(\"\\nTrain set species counts:\\n\", train_counts)\n",
    "print(\"\\nVal set species counts:\\n\", val_counts)\n",
    "print(\"\\nTest set species counts:\\n\", test_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0714e144-ef88-4eae-83da-fce147f3fdad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RamanTransformerClassifier(\n",
      "  (value_embedding): Linear(in_features=1, out_features=128, bias=True)\n",
      "  (transformer_encoder): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-3): 4 x TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "        (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
      "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.2, inplace=False)\n",
      "        (dropout2): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (classifier): Linear(in_features=128, out_features=36, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class RamanTransformerClassifier(nn.Module):\n",
    "    def __init__(self, seq_len, num_classes, d_model=64, nhead=8, dim_feedforward=256, num_layers=2, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Initializes the Transformer-based classifier.\n",
    "        seq_len: length of the input sequence (number of wavelength points, e.g., 1000)\n",
    "        num_classes: number of output classes (species)\n",
    "        d_model: dimensionality of the embedding/transformer model\n",
    "        nhead: number of heads in multi-head attention\n",
    "        dim_feedforward: dimensionality of the feedforward network in transformer layers\n",
    "        num_layers: number of transformer encoder layers\n",
    "        dropout: dropout rate for transformer layers\n",
    "        \"\"\"\n",
    "        super(RamanTransformerClassifier, self).__init__()\n",
    "        # Linear projection of input scalar to d_model-dimensional embedding\n",
    "        self.value_embedding = nn.Linear(1, d_model)\n",
    "        # Positional encoding (sinusoidal)\n",
    "        pos_encoding = torch.zeros(seq_len, d_model)\n",
    "        positions = torch.arange(0, seq_len, dtype=torch.float32).unsqueeze(1)  # shape [seq_len, 1]\n",
    "        # Frequencies for sine/cosine positional encoding\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2, dtype=torch.float32) * -(math.log(10000.0) / d_model))\n",
    "        pos_encoding[:, 0::2] = torch.sin(positions * div_term)  # apply sin to even indices in the embedding\n",
    "        pos_encoding[:, 1::2] = torch.cos(positions * div_term)  # apply cos to odd indices\n",
    "        pos_encoding = pos_encoding.unsqueeze(0)  # shape [1, seq_len, d_model] for broadcasting\n",
    "        # Register as buffer (not a parameter, but part of the state for usage)\n",
    "        self.register_buffer('pos_encoding', pos_encoding)\n",
    "\n",
    "        # Transformer Encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, \n",
    "                                                  dropout=dropout, batch_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        # Classification head\n",
    "        self.classifier = nn.Linear(d_model, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass for the model.\n",
    "        x: input tensor of shape [batch_size, seq_len] containing normalized Raman intensities.\n",
    "        \"\"\"\n",
    "        # Project each intensity value to a d_model-dimensional embedding\n",
    "        # x is [batch, seq_len]; convert to [batch, seq_len, 1] for linear layer\n",
    "        x = x.unsqueeze(-1)\n",
    "        x = self.value_embedding(x)  # now x shape: [batch, seq_len, d_model]\n",
    "\n",
    "        # Add positional encoding to include wavelength position information\n",
    "        x = x + self.pos_encoding[:, :x.size(1), :]  # broadcasting over batch dimension\n",
    "\n",
    "        # Pass through Transformer encoder layers\n",
    "        x = self.transformer_encoder(x)  # shape remains [batch, seq_len, d_model]\n",
    "\n",
    "        # Global average pooling over the sequence dimension\n",
    "        x = x.mean(dim=1)  # shape: [batch, d_model]\n",
    "\n",
    "        # Final classifier to get class logits\n",
    "        logits = self.classifier(x)  # shape: [batch, num_classes]\n",
    "\n",
    "        return logits\n",
    "\n",
    "# Initialize the model\n",
    "seq_length = X_train.shape[1]  # this should be 1000 (number of wavelength points)\n",
    "num_classes = len(unique_species)\n",
    "model = RamanTransformerClassifier(\n",
    "    seq_len=seq_length,                                   \n",
    "    num_classes=num_classes,                                   \n",
    "    d_model=128,                                    \n",
    "    nhead=8,                                    \n",
    "    dim_feedforward=512,                                   \n",
    "    num_layers=4, \n",
    "    dropout=0.2)\n",
    "\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b8cde34-d9f0-46ef-a586-530df5df3ef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Set device for training (GPU if available)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Using device:\", device)\n",
    "model.to(device)\n",
    "\n",
    "# Create DataLoader for batch training\n",
    "batch_size = 512\n",
    "train_dataset = torch.utils.data.TensorDataset(torch.tensor(X_train, dtype=torch.float32), \n",
    "                                               torch.tensor(y_train, dtype=torch.long))\n",
    "val_dataset   = torch.utils.data.TensorDataset(torch.tensor(X_val, dtype=torch.float32), \n",
    "                                               torch.tensor(y_val, dtype=torch.long))\n",
    "test_dataset  = torch.utils.data.TensorDataset(torch.tensor(X_test, dtype=torch.float32), \n",
    "                                               torch.tensor(y_test, dtype=torch.long))\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader   = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader  = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Define optimizer and loss function\n",
    "learning_rate = 1e-3\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate,weight_decay=1e-4)\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "\n",
    "# Training parameters\n",
    "num_epochs = 100\n",
    "patience = 15  # early stopping patience\n",
    "\n",
    "# For reproducibility\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "# Containers to track metrics\n",
    "train_losses, val_losses = [], []\n",
    "train_accuracies, val_accuracies = [], []\n",
    "\n",
    "best_val_acc = 0.0\n",
    "best_model_state = None\n",
    "epochs_no_improve = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d842a9ca-8b5b-4c9c-873b-0e2cf9b60a5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | Train Loss=3.0095, Acc=17.9% | Val Loss=2.4660, Acc=29.5% | LR=1.00e-03\n",
      "Epoch 02 | Train Loss=1.8021, Acc=55.2% | Val Loss=2.0543, Acc=45.8% | LR=1.00e-03\n",
      "Epoch 03 | Train Loss=1.4040, Acc=70.9% | Val Loss=2.0661, Acc=50.9% | LR=1.00e-03\n",
      "Epoch 04 | Train Loss=1.2274, Acc=78.3% | Val Loss=1.9302, Acc=55.1% | LR=1.00e-03\n",
      "Epoch 05 | Train Loss=1.1173, Acc=82.8% | Val Loss=1.8459, Acc=59.9% | LR=1.00e-03\n",
      "Epoch 06 | Train Loss=1.0546, Acc=85.2% | Val Loss=1.7620, Acc=63.8% | LR=1.00e-03\n",
      "Epoch 07 | Train Loss=1.0036, Acc=87.4% | Val Loss=1.7227, Acc=64.9% | LR=1.00e-03\n",
      "Epoch 08 | Train Loss=0.9691, Acc=88.7% | Val Loss=1.7138, Acc=66.4% | LR=1.00e-03\n",
      "Epoch 09 | Train Loss=0.9343, Acc=90.1% | Val Loss=1.7285, Acc=64.7% | LR=1.00e-03\n",
      "Epoch 10 | Train Loss=0.9073, Acc=91.2% | Val Loss=1.8001, Acc=64.6% | LR=1.00e-03\n",
      "Epoch 11 | Train Loss=0.8862, Acc=91.9% | Val Loss=1.7200, Acc=65.7% | LR=1.00e-03\n",
      "Epoch 12 | Train Loss=0.8768, Acc=92.2% | Val Loss=1.7547, Acc=65.9% | LR=1.00e-03\n",
      "Epoch 13 | Train Loss=0.8608, Acc=92.9% | Val Loss=1.8195, Acc=65.2% | LR=1.00e-03\n",
      "Epoch 14 | Train Loss=0.8465, Acc=93.4% | Val Loss=1.7524, Acc=68.1% | LR=1.00e-03\n",
      "Epoch 15 | Train Loss=0.8435, Acc=93.5% | Val Loss=1.8340, Acc=65.4% | LR=1.00e-03\n",
      "Epoch 16 | Train Loss=0.8342, Acc=93.8% | Val Loss=1.7926, Acc=67.3% | LR=1.00e-03\n",
      "Epoch 17 | Train Loss=0.8209, Acc=94.3% | Val Loss=1.6812, Acc=68.5% | LR=1.00e-03\n",
      "Epoch 18 | Train Loss=0.8068, Acc=94.9% | Val Loss=1.7373, Acc=69.5% | LR=1.00e-03\n",
      "Epoch 19 | Train Loss=0.8106, Acc=94.6% | Val Loss=1.6562, Acc=69.3% | LR=1.00e-03\n",
      "Epoch 20 | Train Loss=0.7915, Acc=95.5% | Val Loss=1.7079, Acc=69.0% | LR=1.00e-03\n",
      "Epoch 21 | Train Loss=0.7954, Acc=95.2% | Val Loss=1.7719, Acc=67.9% | LR=1.00e-03\n",
      "Epoch 22 | Train Loss=0.7893, Acc=95.5% | Val Loss=1.8077, Acc=66.7% | LR=1.00e-03\n",
      "Epoch 23 | Train Loss=0.7841, Acc=95.7% | Val Loss=1.6636, Acc=70.3% | LR=1.00e-03\n",
      "Epoch 24 | Train Loss=0.7809, Acc=95.7% | Val Loss=1.8164, Acc=67.6% | LR=1.00e-03\n",
      "Epoch 25 | Train Loss=0.7705, Acc=96.2% | Val Loss=1.8022, Acc=68.6% | LR=1.00e-03\n",
      "Epoch 26 | Train Loss=0.7724, Acc=96.1% | Val Loss=1.8272, Acc=67.5% | LR=1.00e-03\n",
      "Epoch 27 | Train Loss=0.7744, Acc=96.0% | Val Loss=1.7252, Acc=70.2% | LR=1.00e-03\n",
      "Epoch 28 | Train Loss=0.7631, Acc=96.5% | Val Loss=1.8411, Acc=67.4% | LR=5.00e-04\n",
      "Epoch 29 | Train Loss=0.7287, Acc=97.9% | Val Loss=1.7698, Acc=69.6% | LR=5.00e-04\n",
      "Epoch 30 | Train Loss=0.7223, Acc=98.2% | Val Loss=1.7251, Acc=70.3% | LR=5.00e-04\n",
      "Epoch 31 | Train Loss=0.7187, Acc=98.3% | Val Loss=1.7503, Acc=70.7% | LR=5.00e-04\n",
      "Epoch 32 | Train Loss=0.7174, Acc=98.4% | Val Loss=1.7654, Acc=70.7% | LR=5.00e-04\n",
      "Epoch 33 | Train Loss=0.7173, Acc=98.3% | Val Loss=1.7664, Acc=70.7% | LR=5.00e-04\n",
      "Epoch 34 | Train Loss=0.7188, Acc=98.3% | Val Loss=1.7810, Acc=70.0% | LR=5.00e-04\n",
      "Epoch 35 | Train Loss=0.7180, Acc=98.3% | Val Loss=1.7676, Acc=71.2% | LR=5.00e-04\n",
      "Epoch 36 | Train Loss=0.7110, Acc=98.6% | Val Loss=1.7670, Acc=71.5% | LR=5.00e-04\n",
      "Epoch 37 | Train Loss=0.7156, Acc=98.4% | Val Loss=1.7865, Acc=71.0% | LR=2.50e-04\n",
      "Epoch 38 | Train Loss=0.6982, Acc=99.1% | Val Loss=1.7365, Acc=72.1% | LR=2.50e-04\n",
      "Epoch 39 | Train Loss=0.6954, Acc=99.2% | Val Loss=1.7399, Acc=72.3% | LR=2.50e-04\n",
      "Epoch 40 | Train Loss=0.6935, Acc=99.3% | Val Loss=1.7373, Acc=72.6% | LR=2.50e-04\n",
      "Epoch 41 | Train Loss=0.6935, Acc=99.3% | Val Loss=1.7461, Acc=72.6% | LR=2.50e-04\n",
      "Epoch 42 | Train Loss=0.6917, Acc=99.4% | Val Loss=1.7635, Acc=71.9% | LR=2.50e-04\n",
      "Epoch 43 | Train Loss=0.6924, Acc=99.3% | Val Loss=1.7579, Acc=72.5% | LR=2.50e-04\n",
      "Epoch 44 | Train Loss=0.6904, Acc=99.4% | Val Loss=1.7794, Acc=71.7% | LR=2.50e-04\n",
      "Epoch 45 | Train Loss=0.6918, Acc=99.4% | Val Loss=1.7428, Acc=72.8% | LR=2.50e-04\n",
      "Epoch 46 | Train Loss=0.6918, Acc=99.3% | Val Loss=1.7971, Acc=71.9% | LR=1.25e-04\n",
      "Epoch 47 | Train Loss=0.6849, Acc=99.6% | Val Loss=1.7718, Acc=72.1% | LR=1.25e-04\n",
      "Epoch 48 | Train Loss=0.6832, Acc=99.7% | Val Loss=1.7783, Acc=72.1% | LR=1.25e-04\n",
      "Epoch 49 | Train Loss=0.6833, Acc=99.7% | Val Loss=1.7879, Acc=72.0% | LR=1.25e-04\n",
      "Epoch 50 | Train Loss=0.6825, Acc=99.7% | Val Loss=1.7577, Acc=72.5% | LR=1.25e-04\n",
      "Epoch 51 | Train Loss=0.6819, Acc=99.7% | Val Loss=1.7672, Acc=72.4% | LR=1.25e-04\n",
      "Epoch 52 | Train Loss=0.6817, Acc=99.7% | Val Loss=1.7678, Acc=72.5% | LR=1.25e-04\n",
      "Epoch 53 | Train Loss=0.6818, Acc=99.7% | Val Loss=1.7834, Acc=72.4% | LR=1.25e-04\n",
      "Epoch 54 | Train Loss=0.6823, Acc=99.7% | Val Loss=1.7850, Acc=72.3% | LR=1.25e-04\n",
      "Epoch 55 | Train Loss=0.6811, Acc=99.8% | Val Loss=1.8179, Acc=72.0% | LR=6.25e-05\n",
      "Epoch 56 | Train Loss=0.6790, Acc=99.8% | Val Loss=1.7827, Acc=72.1% | LR=6.25e-05\n",
      "Epoch 57 | Train Loss=0.6786, Acc=99.8% | Val Loss=1.7758, Acc=72.3% | LR=6.25e-05\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 33\u001b[39m\n\u001b[32m     30\u001b[39m loss.backward()\n\u001b[32m     31\u001b[39m optimizer.step()\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m train_loss += \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m * X_batch.size(\u001b[32m0\u001b[39m)\n\u001b[32m     34\u001b[39m preds = logits.argmax(dim=\u001b[32m1\u001b[39m)\n\u001b[32m     35\u001b[39m train_correct += (preds == y_batch).sum().item()\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# 0) Set up optimizer + Plateau scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "scheduler = ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode='min',\n",
    "    factor=0.5,\n",
    "    patience=8\n",
    ")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "\n",
    "best_val_acc = 0.0\n",
    "epochs_no_improve = 0\n",
    "\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    # --- training ---\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(X_batch)\n",
    "        loss = criterion(logits, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item() * X_batch.size(0)\n",
    "        preds = logits.argmax(dim=1)\n",
    "        train_correct += (preds == y_batch).sum().item()\n",
    "        train_total += y_batch.size(0)\n",
    "\n",
    "    avg_train_loss = train_loss / train_total\n",
    "    train_acc = train_correct / train_total\n",
    "\n",
    "    # --- validation ---\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            logits = model(X_batch)\n",
    "            loss = criterion(logits, y_batch)\n",
    "            val_loss += loss.item() * X_batch.size(0)\n",
    "            preds = logits.argmax(dim=1)\n",
    "            val_correct += (preds == y_batch).sum().item()\n",
    "            val_total += y_batch.size(0)\n",
    "\n",
    "    avg_val_loss = val_loss / val_total\n",
    "    val_acc = val_correct / val_total\n",
    "\n",
    "    # --- step the scheduler on validation loss ---\n",
    "    scheduler.step(avg_val_loss)\n",
    "\n",
    "    # --- logging ---\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    print(f\"Epoch {epoch:02d} | \"\n",
    "          f\"Train Loss={avg_train_loss:.4f}, Acc={train_acc*100:.1f}% | \"\n",
    "          f\"Val Loss={avg_val_loss:.4f}, Acc={val_acc*100:.1f}% | \"\n",
    "          f\"LR={current_lr:.2e}\")\n",
    "\n",
    "    # --- early stopping on val‐accuracy (unchanged) ---\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        best_model_state = model.state_dict()\n",
    "        epochs_no_improve = 0\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f\"No improvement in validation accuracy for {patience} epochs. Stopping.\")\n",
    "            break\n",
    "\n",
    "# restore the best model\n",
    "model.load_state_dict(best_model_state)\n",
    "print(f\"Loaded best model (Val Acc = {best_val_acc*100:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e576f03-a429-4e99-be49-90e1375eff8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "epochs_ran = len(train_losses)  # total epochs that ran (could be less than num_epochs if early stopped)\n",
    "epoch_range = range(1, epochs_ran+1)\n",
    "\n",
    "# Plot Training and Validation Loss\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(epoch_range, train_losses, label='Training Loss')\n",
    "plt.plot(epoch_range, val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training vs. Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot Training and Validation Accuracy\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(epoch_range, np.array(train_accuracies)*100, label='Training Accuracy')\n",
    "plt.plot(epoch_range, np.array(val_accuracies)*100, label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.title('Training vs. Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80e569d-3b77-42c1-98c9-6cf6be05311c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Build a species->group map\n",
    "species_group_map = df.drop_duplicates('ID').set_index('ID')['Group'].to_dict()\n",
    "\n",
    "# Order and colors\n",
    "group_order  = ['Neg','Pos','Fungi']\n",
    "group_colors = {'Neg':'red','Pos':'gold','Fungi':'green'}\n",
    "\n",
    "# Create the ordered species list\n",
    "species_ordered = [\n",
    "    sp\n",
    "    for grp in group_order\n",
    "    for sp in unique_species\n",
    "    if species_group_map[sp] == grp\n",
    "]\n",
    "\n",
    "# Compute and normalize the confusion matrix\n",
    "cm_counts = confusion_matrix(y_true, y_pred, labels=np.arange(len(unique_species)))\n",
    "row_sums  = cm_counts.sum(axis=1, keepdims=True)\n",
    "cm_pct    = np.nan_to_num(cm_counts / row_sums * 100)  # fill 0/0 with 0\n",
    "\n",
    "# Reorder rows & columns\n",
    "idx       = [unique_species.index(sp) for sp in species_ordered]\n",
    "cm_pct    = cm_pct[np.ix_(idx, idx)]\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(15,12))\n",
    "sns.heatmap(\n",
    "    cm_pct,\n",
    "    annot=True,\n",
    "    fmt=\".1f\",\n",
    "    cmap=\"Blues\",\n",
    "    xticklabels=species_ordered,\n",
    "    yticklabels=species_ordered,\n",
    "    cbar_kws={'label':'% of true class'},\n",
    "    ax=ax\n",
    ")\n",
    "\n",
    "# Color tick labels by group\n",
    "plt.xticks(rotation=90)\n",
    "plt.yticks(rotation=0)\n",
    "for lbl in ax.get_xticklabels():\n",
    "    lbl.set_color(group_colors[species_group_map[lbl.get_text()]])\n",
    "for lbl in ax.get_yticklabels():\n",
    "    lbl.set_color(group_colors[species_group_map[lbl.get_text()]])\n",
    "\n",
    "# Draw separators between Neg/Pos/Fungi blocks\n",
    "neg_end = sum(1 for sp in species_ordered if species_group_map[sp]=='Neg')\n",
    "pos_end = neg_end + sum(1 for sp in species_ordered if species_group_map[sp]=='Pos')\n",
    "ax.hlines([neg_end, pos_end], *ax.get_xlim(), colors='black', linewidth=2)\n",
    "ax.vlines([neg_end, pos_end], *ax.get_ylim(), colors='black', linewidth=2)\n",
    "\n",
    "# Labels & title\n",
    "ax.set_xlabel(\"Predicted Species\")\n",
    "ax.set_ylabel(\"True Species\")\n",
    "ax.set_title(\"Normalized Confusion Matrix (%) Grouped by Gram-Neg / Gram-Pos / Fungi\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e31395-d0f9-4157-bd17-e9cb55a9c601",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
